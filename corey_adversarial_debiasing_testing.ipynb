{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 10:23:16.866774: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/fair-ML/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 10:23:31.704536: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-10 10:23:31.964265: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.691865\n",
      "epoch 0; iter: 200; batch classifier loss: 0.399723\n",
      "epoch 1; iter: 0; batch classifier loss: 0.373779\n",
      "epoch 1; iter: 200; batch classifier loss: 0.446052\n",
      "epoch 2; iter: 0; batch classifier loss: 0.433118\n",
      "epoch 2; iter: 200; batch classifier loss: 0.452567\n",
      "epoch 3; iter: 0; batch classifier loss: 0.451564\n",
      "epoch 3; iter: 200; batch classifier loss: 0.364452\n",
      "epoch 4; iter: 0; batch classifier loss: 0.316409\n",
      "epoch 4; iter: 200; batch classifier loss: 0.420796\n",
      "epoch 5; iter: 0; batch classifier loss: 0.460488\n",
      "epoch 5; iter: 200; batch classifier loss: 0.325201\n",
      "epoch 6; iter: 0; batch classifier loss: 0.403974\n",
      "epoch 6; iter: 200; batch classifier loss: 0.510813\n",
      "epoch 7; iter: 0; batch classifier loss: 0.400003\n",
      "epoch 7; iter: 200; batch classifier loss: 0.419344\n",
      "epoch 8; iter: 0; batch classifier loss: 0.393383\n",
      "epoch 8; iter: 200; batch classifier loss: 0.381345\n",
      "epoch 9; iter: 0; batch classifier loss: 0.515684\n",
      "epoch 9; iter: 200; batch classifier loss: 0.403808\n",
      "epoch 10; iter: 0; batch classifier loss: 0.408753\n",
      "epoch 10; iter: 200; batch classifier loss: 0.448358\n",
      "epoch 11; iter: 0; batch classifier loss: 0.418435\n",
      "epoch 11; iter: 200; batch classifier loss: 0.429703\n",
      "epoch 12; iter: 0; batch classifier loss: 0.443432\n",
      "epoch 12; iter: 200; batch classifier loss: 0.417284\n",
      "epoch 13; iter: 0; batch classifier loss: 0.417797\n",
      "epoch 13; iter: 200; batch classifier loss: 0.420482\n",
      "epoch 14; iter: 0; batch classifier loss: 0.474972\n",
      "epoch 14; iter: 200; batch classifier loss: 0.377577\n",
      "epoch 15; iter: 0; batch classifier loss: 0.463426\n",
      "epoch 15; iter: 200; batch classifier loss: 0.396892\n",
      "epoch 16; iter: 0; batch classifier loss: 0.416212\n",
      "epoch 16; iter: 200; batch classifier loss: 0.320890\n",
      "epoch 17; iter: 0; batch classifier loss: 0.372620\n",
      "epoch 17; iter: 200; batch classifier loss: 0.404022\n",
      "epoch 18; iter: 0; batch classifier loss: 0.444276\n",
      "epoch 18; iter: 200; batch classifier loss: 0.419034\n",
      "epoch 19; iter: 0; batch classifier loss: 0.425203\n",
      "epoch 19; iter: 200; batch classifier loss: 0.439141\n",
      "epoch 20; iter: 0; batch classifier loss: 0.368063\n",
      "epoch 20; iter: 200; batch classifier loss: 0.456539\n",
      "epoch 21; iter: 0; batch classifier loss: 0.454395\n",
      "epoch 21; iter: 200; batch classifier loss: 0.417918\n",
      "epoch 22; iter: 0; batch classifier loss: 0.379188\n",
      "epoch 22; iter: 200; batch classifier loss: 0.360342\n",
      "epoch 23; iter: 0; batch classifier loss: 0.394910\n",
      "epoch 23; iter: 200; batch classifier loss: 0.458355\n",
      "epoch 24; iter: 0; batch classifier loss: 0.386614\n",
      "epoch 24; iter: 200; batch classifier loss: 0.369395\n",
      "epoch 25; iter: 0; batch classifier loss: 0.393678\n",
      "epoch 25; iter: 200; batch classifier loss: 0.408573\n",
      "epoch 26; iter: 0; batch classifier loss: 0.387836\n",
      "epoch 26; iter: 200; batch classifier loss: 0.502430\n",
      "epoch 27; iter: 0; batch classifier loss: 0.365878\n",
      "epoch 27; iter: 200; batch classifier loss: 0.484218\n",
      "epoch 28; iter: 0; batch classifier loss: 0.379059\n",
      "epoch 28; iter: 200; batch classifier loss: 0.450273\n",
      "epoch 29; iter: 0; batch classifier loss: 0.401121\n",
      "epoch 29; iter: 200; batch classifier loss: 0.506336\n",
      "epoch 30; iter: 0; batch classifier loss: 0.379075\n",
      "epoch 30; iter: 200; batch classifier loss: 0.490765\n",
      "epoch 31; iter: 0; batch classifier loss: 0.364467\n",
      "epoch 31; iter: 200; batch classifier loss: 0.454673\n",
      "epoch 32; iter: 0; batch classifier loss: 0.501406\n",
      "epoch 32; iter: 200; batch classifier loss: 0.480182\n",
      "epoch 33; iter: 0; batch classifier loss: 0.461732\n",
      "epoch 33; iter: 200; batch classifier loss: 0.421375\n",
      "epoch 34; iter: 0; batch classifier loss: 0.436689\n",
      "epoch 34; iter: 200; batch classifier loss: 0.422168\n",
      "epoch 35; iter: 0; batch classifier loss: 0.429885\n",
      "epoch 35; iter: 200; batch classifier loss: 0.464026\n",
      "epoch 36; iter: 0; batch classifier loss: 0.347361\n",
      "epoch 36; iter: 200; batch classifier loss: 0.423735\n",
      "epoch 37; iter: 0; batch classifier loss: 0.447112\n",
      "epoch 37; iter: 200; batch classifier loss: 0.356799\n",
      "epoch 38; iter: 0; batch classifier loss: 0.371898\n",
      "epoch 38; iter: 200; batch classifier loss: 0.371035\n",
      "epoch 39; iter: 0; batch classifier loss: 0.432174\n",
      "epoch 39; iter: 200; batch classifier loss: 0.399867\n",
      "epoch 40; iter: 0; batch classifier loss: 0.507782\n",
      "epoch 40; iter: 200; batch classifier loss: 0.441210\n",
      "epoch 41; iter: 0; batch classifier loss: 0.405957\n",
      "epoch 41; iter: 200; batch classifier loss: 0.467230\n",
      "epoch 42; iter: 0; batch classifier loss: 0.395957\n",
      "epoch 42; iter: 200; batch classifier loss: 0.464065\n",
      "epoch 43; iter: 0; batch classifier loss: 0.327636\n",
      "epoch 43; iter: 200; batch classifier loss: 0.464356\n",
      "epoch 44; iter: 0; batch classifier loss: 0.384569\n",
      "epoch 44; iter: 200; batch classifier loss: 0.479486\n",
      "epoch 45; iter: 0; batch classifier loss: 0.403299\n",
      "epoch 45; iter: 200; batch classifier loss: 0.459796\n",
      "epoch 46; iter: 0; batch classifier loss: 0.397211\n",
      "epoch 46; iter: 200; batch classifier loss: 0.359562\n",
      "epoch 47; iter: 0; batch classifier loss: 0.447779\n",
      "epoch 47; iter: 200; batch classifier loss: 0.466256\n",
      "epoch 48; iter: 0; batch classifier loss: 0.345806\n",
      "epoch 48; iter: 200; batch classifier loss: 0.430050\n",
      "epoch 49; iter: 0; batch classifier loss: 0.388293\n",
      "epoch 49; iter: 200; batch classifier loss: 0.382093\n",
      "epoch 0; iter: 0; batch classifier loss: 0.680502; batch adversarial loss: 0.643456\n",
      "epoch 0; iter: 200; batch classifier loss: 0.423535; batch adversarial loss: 0.667888\n",
      "epoch 1; iter: 0; batch classifier loss: 0.545092; batch adversarial loss: 0.648465\n",
      "epoch 1; iter: 200; batch classifier loss: 0.577009; batch adversarial loss: 0.640236\n",
      "epoch 2; iter: 0; batch classifier loss: 0.519437; batch adversarial loss: 0.627820\n",
      "epoch 2; iter: 200; batch classifier loss: 0.414119; batch adversarial loss: 0.678202\n",
      "epoch 3; iter: 0; batch classifier loss: 0.434464; batch adversarial loss: 0.647696\n",
      "epoch 3; iter: 200; batch classifier loss: 0.461285; batch adversarial loss: 0.610662\n",
      "epoch 4; iter: 0; batch classifier loss: 0.508431; batch adversarial loss: 0.594853\n",
      "epoch 4; iter: 200; batch classifier loss: 0.367523; batch adversarial loss: 0.628552\n",
      "epoch 5; iter: 0; batch classifier loss: 0.435559; batch adversarial loss: 0.617438\n",
      "epoch 5; iter: 200; batch classifier loss: 0.508934; batch adversarial loss: 0.635929\n",
      "epoch 6; iter: 0; batch classifier loss: 0.494026; batch adversarial loss: 0.610084\n",
      "epoch 6; iter: 200; batch classifier loss: 0.433973; batch adversarial loss: 0.609660\n",
      "epoch 7; iter: 0; batch classifier loss: 0.484287; batch adversarial loss: 0.573000\n",
      "epoch 7; iter: 200; batch classifier loss: 0.496725; batch adversarial loss: 0.586140\n",
      "epoch 8; iter: 0; batch classifier loss: 0.496618; batch adversarial loss: 0.595516\n",
      "epoch 8; iter: 200; batch classifier loss: 0.467765; batch adversarial loss: 0.713715\n",
      "epoch 9; iter: 0; batch classifier loss: 0.494956; batch adversarial loss: 0.597566\n",
      "epoch 9; iter: 200; batch classifier loss: 0.394177; batch adversarial loss: 0.643344\n",
      "epoch 10; iter: 0; batch classifier loss: 0.370821; batch adversarial loss: 0.621390\n",
      "epoch 10; iter: 200; batch classifier loss: 0.370548; batch adversarial loss: 0.610316\n",
      "epoch 11; iter: 0; batch classifier loss: 0.451980; batch adversarial loss: 0.601749\n",
      "epoch 11; iter: 200; batch classifier loss: 0.431457; batch adversarial loss: 0.616905\n",
      "epoch 12; iter: 0; batch classifier loss: 0.374596; batch adversarial loss: 0.647650\n",
      "epoch 12; iter: 200; batch classifier loss: 0.528418; batch adversarial loss: 0.597839\n",
      "epoch 13; iter: 0; batch classifier loss: 0.396161; batch adversarial loss: 0.650536\n",
      "epoch 13; iter: 200; batch classifier loss: 0.454500; batch adversarial loss: 0.553683\n",
      "epoch 14; iter: 0; batch classifier loss: 0.471516; batch adversarial loss: 0.622627\n",
      "epoch 14; iter: 200; batch classifier loss: 0.420605; batch adversarial loss: 0.659600\n",
      "epoch 15; iter: 0; batch classifier loss: 0.476515; batch adversarial loss: 0.606036\n",
      "epoch 15; iter: 200; batch classifier loss: 0.508290; batch adversarial loss: 0.628242\n",
      "epoch 16; iter: 0; batch classifier loss: 0.528752; batch adversarial loss: 0.587059\n",
      "epoch 16; iter: 200; batch classifier loss: 0.506880; batch adversarial loss: 0.607503\n",
      "epoch 17; iter: 0; batch classifier loss: 0.457194; batch adversarial loss: 0.679028\n",
      "epoch 17; iter: 200; batch classifier loss: 0.430263; batch adversarial loss: 0.614590\n",
      "epoch 18; iter: 0; batch classifier loss: 0.469217; batch adversarial loss: 0.625494\n",
      "epoch 18; iter: 200; batch classifier loss: 0.373297; batch adversarial loss: 0.661629\n",
      "epoch 19; iter: 0; batch classifier loss: 0.436996; batch adversarial loss: 0.586590\n",
      "epoch 19; iter: 200; batch classifier loss: 0.467239; batch adversarial loss: 0.608667\n",
      "epoch 20; iter: 0; batch classifier loss: 0.473501; batch adversarial loss: 0.632577\n",
      "epoch 20; iter: 200; batch classifier loss: 0.404757; batch adversarial loss: 0.572109\n",
      "epoch 21; iter: 0; batch classifier loss: 0.406823; batch adversarial loss: 0.625480\n",
      "epoch 21; iter: 200; batch classifier loss: 0.471669; batch adversarial loss: 0.598303\n",
      "epoch 22; iter: 0; batch classifier loss: 0.433841; batch adversarial loss: 0.590650\n",
      "epoch 22; iter: 200; batch classifier loss: 0.424953; batch adversarial loss: 0.635109\n",
      "epoch 23; iter: 0; batch classifier loss: 0.420048; batch adversarial loss: 0.645063\n",
      "epoch 23; iter: 200; batch classifier loss: 0.482824; batch adversarial loss: 0.628935\n",
      "epoch 24; iter: 0; batch classifier loss: 0.487393; batch adversarial loss: 0.554856\n",
      "epoch 24; iter: 200; batch classifier loss: 0.417759; batch adversarial loss: 0.549071\n",
      "epoch 25; iter: 0; batch classifier loss: 0.457467; batch adversarial loss: 0.597148\n",
      "epoch 25; iter: 200; batch classifier loss: 0.431034; batch adversarial loss: 0.587563\n",
      "epoch 26; iter: 0; batch classifier loss: 0.416147; batch adversarial loss: 0.565216\n",
      "epoch 26; iter: 200; batch classifier loss: 0.462822; batch adversarial loss: 0.587072\n",
      "epoch 27; iter: 0; batch classifier loss: 0.428378; batch adversarial loss: 0.623731\n",
      "epoch 27; iter: 200; batch classifier loss: 0.396183; batch adversarial loss: 0.629772\n",
      "epoch 28; iter: 0; batch classifier loss: 0.325412; batch adversarial loss: 0.598060\n",
      "epoch 28; iter: 200; batch classifier loss: 0.462397; batch adversarial loss: 0.659059\n",
      "epoch 29; iter: 0; batch classifier loss: 0.478200; batch adversarial loss: 0.641834\n",
      "epoch 29; iter: 200; batch classifier loss: 0.403049; batch adversarial loss: 0.610887\n",
      "epoch 30; iter: 0; batch classifier loss: 0.447776; batch adversarial loss: 0.620664\n",
      "epoch 30; iter: 200; batch classifier loss: 0.412534; batch adversarial loss: 0.621782\n",
      "epoch 31; iter: 0; batch classifier loss: 0.369785; batch adversarial loss: 0.594482\n",
      "epoch 31; iter: 200; batch classifier loss: 0.481056; batch adversarial loss: 0.627975\n",
      "epoch 32; iter: 0; batch classifier loss: 0.402282; batch adversarial loss: 0.598892\n",
      "epoch 32; iter: 200; batch classifier loss: 0.488541; batch adversarial loss: 0.661011\n",
      "epoch 33; iter: 0; batch classifier loss: 0.440805; batch adversarial loss: 0.651672\n",
      "epoch 33; iter: 200; batch classifier loss: 0.402854; batch adversarial loss: 0.673555\n",
      "epoch 34; iter: 0; batch classifier loss: 0.422837; batch adversarial loss: 0.621998\n",
      "epoch 34; iter: 200; batch classifier loss: 0.480778; batch adversarial loss: 0.604432\n",
      "epoch 35; iter: 0; batch classifier loss: 0.311912; batch adversarial loss: 0.600326\n",
      "epoch 35; iter: 200; batch classifier loss: 0.418272; batch adversarial loss: 0.617474\n",
      "epoch 36; iter: 0; batch classifier loss: 0.371506; batch adversarial loss: 0.568229\n",
      "epoch 36; iter: 200; batch classifier loss: 0.380931; batch adversarial loss: 0.584679\n",
      "epoch 37; iter: 0; batch classifier loss: 0.429804; batch adversarial loss: 0.647382\n",
      "epoch 37; iter: 200; batch classifier loss: 0.551299; batch adversarial loss: 0.613509\n",
      "epoch 38; iter: 0; batch classifier loss: 0.464550; batch adversarial loss: 0.532591\n",
      "epoch 38; iter: 200; batch classifier loss: 0.415184; batch adversarial loss: 0.586586\n",
      "epoch 39; iter: 0; batch classifier loss: 0.432673; batch adversarial loss: 0.579133\n",
      "epoch 39; iter: 200; batch classifier loss: 0.563109; batch adversarial loss: 0.629825\n",
      "epoch 40; iter: 0; batch classifier loss: 0.449937; batch adversarial loss: 0.563501\n",
      "epoch 40; iter: 200; batch classifier loss: 0.445865; batch adversarial loss: 0.660215\n",
      "epoch 41; iter: 0; batch classifier loss: 0.396290; batch adversarial loss: 0.582693\n",
      "epoch 41; iter: 200; batch classifier loss: 0.481273; batch adversarial loss: 0.546684\n",
      "epoch 42; iter: 0; batch classifier loss: 0.459934; batch adversarial loss: 0.594151\n",
      "epoch 42; iter: 200; batch classifier loss: 0.472261; batch adversarial loss: 0.633266\n",
      "epoch 43; iter: 0; batch classifier loss: 0.496242; batch adversarial loss: 0.631505\n",
      "epoch 43; iter: 200; batch classifier loss: 0.502937; batch adversarial loss: 0.538064\n",
      "epoch 44; iter: 0; batch classifier loss: 0.424889; batch adversarial loss: 0.607981\n",
      "epoch 44; iter: 200; batch classifier loss: 0.378588; batch adversarial loss: 0.622245\n",
      "epoch 45; iter: 0; batch classifier loss: 0.491044; batch adversarial loss: 0.643580\n",
      "epoch 45; iter: 200; batch classifier loss: 0.422068; batch adversarial loss: 0.561670\n",
      "epoch 46; iter: 0; batch classifier loss: 0.410687; batch adversarial loss: 0.561550\n",
      "epoch 46; iter: 200; batch classifier loss: 0.462592; batch adversarial loss: 0.604876\n",
      "epoch 47; iter: 0; batch classifier loss: 0.408278; batch adversarial loss: 0.577276\n",
      "epoch 47; iter: 200; batch classifier loss: 0.431692; batch adversarial loss: 0.599801\n",
      "epoch 48; iter: 0; batch classifier loss: 0.428635; batch adversarial loss: 0.576043\n",
      "epoch 48; iter: 200; batch classifier loss: 0.522375; batch adversarial loss: 0.563355\n",
      "epoch 49; iter: 0; batch classifier loss: 0.415765; batch adversarial loss: 0.643580\n",
      "epoch 49; iter: 200; batch classifier loss: 0.494097; batch adversarial loss: 0.550853\n"
     ]
    }
   ],
   "source": [
    "def get_plain_and_debaised_model():\n",
    "    ## trains on whole dataset/ no train-test split in this function\n",
    "    dataset_orig= load_preproc_data_adult()\n",
    "    dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "    privileged_groups = [{'sex': 1}]\n",
    "    unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "    sess_plain = tf.Session()\n",
    "    plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess_plain)\n",
    "\n",
    "    plain_model.fit(dataset_orig_train)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    sess_debiased = tf.Session()\n",
    "    debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess_debiased)\n",
    "\n",
    "    debiased_model.fit(dataset_orig_train)\n",
    "\n",
    "    return plain_model, debiased_model\n",
    "\n",
    "\n",
    "plain_model, debiased_model = get_plain_and_debaised_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_entry_adult(race, sex, age, education_years):\n",
    "    arr = np.array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "    arr[0][0] = race\n",
    "    arr[0][1] = sex\n",
    "    if age <= 19:\n",
    "        arr[0][2] = 1.\n",
    "    elif age <= 29:\n",
    "        arr[0][3] = 1.\n",
    "    elif age <= 39:\n",
    "        arr[0][4] = 1.\n",
    "    elif age <= 49:\n",
    "        arr[0][5] = 1.\n",
    "    elif age <= 59:\n",
    "        arr[0][6] = 1.\n",
    "    elif age <= 69:\n",
    "        arr[0][7] = 1.\n",
    "    else:\n",
    "        arr[0][8] = 1.\n",
    "\n",
    "    if education_years < 6:\n",
    "        arr[0][16] = 1.\n",
    "    elif education_years == 6:\n",
    "        arr[0][9] = 1.\n",
    "    elif education_years == 7:\n",
    "        arr[0][10] = 1.\n",
    "    elif education_years == 8:\n",
    "        arr[0][11] = 1.\n",
    "    elif education_years == 9:\n",
    "        arr[0][12] = 1.\n",
    "    elif education_years == 10:\n",
    "        arr[0][13] = 1.\n",
    "    elif education_years == 11:\n",
    "        arr[0][14] = 1.\n",
    "    elif education_years == 12:\n",
    "        arr[0][15] = 1.\n",
    "    else:\n",
    "        arr[0][17] = 1.\n",
    "    \n",
    "    dataset_replaced_data = load_preproc_data_adult()\n",
    "    dataset_replaced_data.features = arr\n",
    "    dataset_replaced_data.age = age\n",
    "    dataset_replaced_data.edu = education_years\n",
    "    return dataset_replaced_data\n",
    "\n",
    "# IDK what the race and sex is - that is, idk what sex = 1 means or race = 0\n",
    "# not sure where to find that\n",
    "example_input = create_single_entry_adult(1, 0, 65, 13)\n",
    "# we can make a script that prompts a user to input sex, race, age, and education and input to this^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that a 65 year old (race=Asian-Pac-Islander) (sex=Female) with 13 years of education DOES NOT have an income greater than 50k.\n"
     ]
    }
   ],
   "source": [
    "def predict_single_income(model, user_input):\n",
    "    pred = model.predict(user_input).labels[0][0]\n",
    "    \n",
    "    races = ['White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other', 'Black']\n",
    "    genders = ['Female', 'Male']\n",
    "\n",
    "    race_print = f\"(race={races[int(user_input.features[0][0])]})\" # idk which value is which race so this is the placeholder \n",
    "    sex_print = f\"(sex={genders[int(user_input.features[0][1])]})\" # idk which value is which sex so this is the placeholder \n",
    "\n",
    "    if pred == 1.0:\n",
    "        print(f\"The model predicts that a {user_input.age} year old {race_print} {sex_print} with {user_input.edu} years of education DOES have an income greater than 50k.\")\n",
    "    elif pred == 0.0:\n",
    "        print(f\"The model predicts that a {user_input.age} year old {race_print} {sex_print} with {user_input.edu} years of education DOES NOT have an income greater than 50k.\")\n",
    "\n",
    "predict_single_income(plain_model, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that a 65 year old (race=Asian-Pac-Islander) (sex=Male) with 13 years of education DOES have an income greater than 50k.\n",
      "The model predicts that a 65 year old (race=Asian-Pac-Islander) (sex=Female) with 13 years of education DOES NOT have an income greater than 50k.\n"
     ]
    }
   ],
   "source": [
    "input1 = create_single_entry_adult(1, 1, 65, 13)\n",
    "input2 = create_single_entry_adult(1, 0, 65, 13)\n",
    "\n",
    "predict_single_income(plain_model, input1)\n",
    "predict_single_income(plain_model, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that a 65 year old (race=Asian-Pac-Islander) (sex=Male) with 11 years of education DOES NOT have an income greater than 50k.\n",
      "The model predicts that a 65 year old (race=Asian-Pac-Islander) (sex=Female) with 11 years of education DOES NOT have an income greater than 50k.\n"
     ]
    }
   ],
   "source": [
    "input1 = create_single_entry_adult(1, 1, 65, 11)\n",
    "input2 = create_single_entry_adult(1, 0, 65, 11)\n",
    "\n",
    "predict_single_income(debiased_model, input1)\n",
    "predict_single_income(debiased_model, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fair-ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0646381eb061ee7893835682a62ef866835a212f0fa32ea5e46413dcbeb946ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
