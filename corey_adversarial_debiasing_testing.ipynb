{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 20:49:51.971505: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 20:49:52.074932: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-09 20:49:52.486858: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-09 20:49:52.486910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-09 20:49:52.486916: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/corey/miniconda3/envs/aif360/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 20:49:53.224910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-09 20:49:53.246775: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-09 20:49:53.246792: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-09 20:49:53.247157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 20:49:53.362279: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.663739\n",
      "epoch 0; iter: 200; batch classifier loss: 0.421104\n",
      "epoch 1; iter: 0; batch classifier loss: 0.449258\n",
      "epoch 1; iter: 200; batch classifier loss: 0.456448\n",
      "epoch 2; iter: 0; batch classifier loss: 0.495314\n",
      "epoch 2; iter: 200; batch classifier loss: 0.419341\n",
      "epoch 3; iter: 0; batch classifier loss: 0.434218\n",
      "epoch 3; iter: 200; batch classifier loss: 0.391853\n",
      "epoch 4; iter: 0; batch classifier loss: 0.497861\n",
      "epoch 4; iter: 200; batch classifier loss: 0.408778\n",
      "epoch 5; iter: 0; batch classifier loss: 0.401274\n",
      "epoch 5; iter: 200; batch classifier loss: 0.423250\n",
      "epoch 6; iter: 0; batch classifier loss: 0.478650\n",
      "epoch 6; iter: 200; batch classifier loss: 0.405312\n",
      "epoch 7; iter: 0; batch classifier loss: 0.404838\n",
      "epoch 7; iter: 200; batch classifier loss: 0.386942\n",
      "epoch 8; iter: 0; batch classifier loss: 0.392513\n",
      "epoch 8; iter: 200; batch classifier loss: 0.460079\n",
      "epoch 9; iter: 0; batch classifier loss: 0.443524\n",
      "epoch 9; iter: 200; batch classifier loss: 0.364426\n",
      "epoch 10; iter: 0; batch classifier loss: 0.363997\n",
      "epoch 10; iter: 200; batch classifier loss: 0.513212\n",
      "epoch 11; iter: 0; batch classifier loss: 0.370950\n",
      "epoch 11; iter: 200; batch classifier loss: 0.379580\n",
      "epoch 12; iter: 0; batch classifier loss: 0.403821\n",
      "epoch 12; iter: 200; batch classifier loss: 0.374102\n",
      "epoch 13; iter: 0; batch classifier loss: 0.417822\n",
      "epoch 13; iter: 200; batch classifier loss: 0.367342\n",
      "epoch 14; iter: 0; batch classifier loss: 0.383553\n",
      "epoch 14; iter: 200; batch classifier loss: 0.417814\n",
      "epoch 15; iter: 0; batch classifier loss: 0.370409\n",
      "epoch 15; iter: 200; batch classifier loss: 0.507645\n",
      "epoch 16; iter: 0; batch classifier loss: 0.304797\n",
      "epoch 16; iter: 200; batch classifier loss: 0.386948\n",
      "epoch 17; iter: 0; batch classifier loss: 0.463337\n",
      "epoch 17; iter: 200; batch classifier loss: 0.357700\n",
      "epoch 18; iter: 0; batch classifier loss: 0.505607\n",
      "epoch 18; iter: 200; batch classifier loss: 0.441426\n",
      "epoch 19; iter: 0; batch classifier loss: 0.387692\n",
      "epoch 19; iter: 200; batch classifier loss: 0.407918\n",
      "epoch 20; iter: 0; batch classifier loss: 0.421019\n",
      "epoch 20; iter: 200; batch classifier loss: 0.310312\n",
      "epoch 21; iter: 0; batch classifier loss: 0.474186\n",
      "epoch 21; iter: 200; batch classifier loss: 0.402563\n",
      "epoch 22; iter: 0; batch classifier loss: 0.496216\n",
      "epoch 22; iter: 200; batch classifier loss: 0.447508\n",
      "epoch 23; iter: 0; batch classifier loss: 0.351827\n",
      "epoch 23; iter: 200; batch classifier loss: 0.403907\n",
      "epoch 24; iter: 0; batch classifier loss: 0.414581\n",
      "epoch 24; iter: 200; batch classifier loss: 0.459268\n",
      "epoch 25; iter: 0; batch classifier loss: 0.428239\n",
      "epoch 25; iter: 200; batch classifier loss: 0.390986\n",
      "epoch 26; iter: 0; batch classifier loss: 0.346218\n",
      "epoch 26; iter: 200; batch classifier loss: 0.427473\n",
      "epoch 27; iter: 0; batch classifier loss: 0.383485\n",
      "epoch 27; iter: 200; batch classifier loss: 0.374655\n",
      "epoch 28; iter: 0; batch classifier loss: 0.424001\n",
      "epoch 28; iter: 200; batch classifier loss: 0.374006\n",
      "epoch 29; iter: 0; batch classifier loss: 0.429620\n",
      "epoch 29; iter: 200; batch classifier loss: 0.400962\n",
      "epoch 30; iter: 0; batch classifier loss: 0.463681\n",
      "epoch 30; iter: 200; batch classifier loss: 0.375479\n",
      "epoch 31; iter: 0; batch classifier loss: 0.407365\n",
      "epoch 31; iter: 200; batch classifier loss: 0.429195\n",
      "epoch 32; iter: 0; batch classifier loss: 0.427250\n",
      "epoch 32; iter: 200; batch classifier loss: 0.470259\n",
      "epoch 33; iter: 0; batch classifier loss: 0.420203\n",
      "epoch 33; iter: 200; batch classifier loss: 0.353022\n",
      "epoch 34; iter: 0; batch classifier loss: 0.434200\n",
      "epoch 34; iter: 200; batch classifier loss: 0.456726\n",
      "epoch 35; iter: 0; batch classifier loss: 0.414510\n",
      "epoch 35; iter: 200; batch classifier loss: 0.416455\n",
      "epoch 36; iter: 0; batch classifier loss: 0.393081\n",
      "epoch 36; iter: 200; batch classifier loss: 0.409037\n",
      "epoch 37; iter: 0; batch classifier loss: 0.404876\n",
      "epoch 37; iter: 200; batch classifier loss: 0.433616\n",
      "epoch 38; iter: 0; batch classifier loss: 0.443598\n",
      "epoch 38; iter: 200; batch classifier loss: 0.393059\n",
      "epoch 39; iter: 0; batch classifier loss: 0.314238\n",
      "epoch 39; iter: 200; batch classifier loss: 0.492834\n",
      "epoch 40; iter: 0; batch classifier loss: 0.465556\n",
      "epoch 40; iter: 200; batch classifier loss: 0.410045\n",
      "epoch 41; iter: 0; batch classifier loss: 0.364159\n",
      "epoch 41; iter: 200; batch classifier loss: 0.455740\n",
      "epoch 42; iter: 0; batch classifier loss: 0.423860\n",
      "epoch 42; iter: 200; batch classifier loss: 0.362739\n",
      "epoch 43; iter: 0; batch classifier loss: 0.437391\n",
      "epoch 43; iter: 200; batch classifier loss: 0.469630\n",
      "epoch 44; iter: 0; batch classifier loss: 0.450871\n",
      "epoch 44; iter: 200; batch classifier loss: 0.385171\n",
      "epoch 45; iter: 0; batch classifier loss: 0.399463\n",
      "epoch 45; iter: 200; batch classifier loss: 0.445962\n",
      "epoch 46; iter: 0; batch classifier loss: 0.474874\n",
      "epoch 46; iter: 200; batch classifier loss: 0.449066\n",
      "epoch 47; iter: 0; batch classifier loss: 0.389642\n",
      "epoch 47; iter: 200; batch classifier loss: 0.486993\n",
      "epoch 48; iter: 0; batch classifier loss: 0.361919\n",
      "epoch 48; iter: 200; batch classifier loss: 0.426413\n",
      "epoch 49; iter: 0; batch classifier loss: 0.544026\n",
      "epoch 49; iter: 200; batch classifier loss: 0.486608\n",
      "epoch 0; iter: 0; batch classifier loss: 0.723212; batch adversarial loss: 0.653875\n",
      "epoch 0; iter: 200; batch classifier loss: 0.407567; batch adversarial loss: 0.641166\n",
      "epoch 1; iter: 0; batch classifier loss: 0.507257; batch adversarial loss: 0.662359\n",
      "epoch 1; iter: 200; batch classifier loss: 0.401615; batch adversarial loss: 0.638832\n",
      "epoch 2; iter: 0; batch classifier loss: 0.486325; batch adversarial loss: 0.652204\n",
      "epoch 2; iter: 200; batch classifier loss: 0.548625; batch adversarial loss: 0.656293\n",
      "epoch 3; iter: 0; batch classifier loss: 0.480514; batch adversarial loss: 0.609583\n",
      "epoch 3; iter: 200; batch classifier loss: 0.497203; batch adversarial loss: 0.638151\n",
      "epoch 4; iter: 0; batch classifier loss: 0.493961; batch adversarial loss: 0.621934\n",
      "epoch 4; iter: 200; batch classifier loss: 0.472079; batch adversarial loss: 0.622256\n",
      "epoch 5; iter: 0; batch classifier loss: 0.447654; batch adversarial loss: 0.581736\n",
      "epoch 5; iter: 200; batch classifier loss: 0.416327; batch adversarial loss: 0.650314\n",
      "epoch 6; iter: 0; batch classifier loss: 0.498821; batch adversarial loss: 0.618945\n",
      "epoch 6; iter: 200; batch classifier loss: 0.414038; batch adversarial loss: 0.640500\n",
      "epoch 7; iter: 0; batch classifier loss: 0.472513; batch adversarial loss: 0.629568\n",
      "epoch 7; iter: 200; batch classifier loss: 0.373569; batch adversarial loss: 0.599620\n",
      "epoch 8; iter: 0; batch classifier loss: 0.436750; batch adversarial loss: 0.616794\n",
      "epoch 8; iter: 200; batch classifier loss: 0.480432; batch adversarial loss: 0.653637\n",
      "epoch 9; iter: 0; batch classifier loss: 0.416375; batch adversarial loss: 0.600139\n",
      "epoch 9; iter: 200; batch classifier loss: 0.370165; batch adversarial loss: 0.602187\n",
      "epoch 10; iter: 0; batch classifier loss: 0.401592; batch adversarial loss: 0.567587\n",
      "epoch 10; iter: 200; batch classifier loss: 0.382103; batch adversarial loss: 0.607286\n",
      "epoch 11; iter: 0; batch classifier loss: 0.450174; batch adversarial loss: 0.622345\n",
      "epoch 11; iter: 200; batch classifier loss: 0.429900; batch adversarial loss: 0.643236\n",
      "epoch 12; iter: 0; batch classifier loss: 0.444787; batch adversarial loss: 0.581167\n",
      "epoch 12; iter: 200; batch classifier loss: 0.434885; batch adversarial loss: 0.601442\n",
      "epoch 13; iter: 0; batch classifier loss: 0.500935; batch adversarial loss: 0.590399\n",
      "epoch 13; iter: 200; batch classifier loss: 0.399726; batch adversarial loss: 0.587026\n",
      "epoch 14; iter: 0; batch classifier loss: 0.389608; batch adversarial loss: 0.621982\n",
      "epoch 14; iter: 200; batch classifier loss: 0.391102; batch adversarial loss: 0.582839\n",
      "epoch 15; iter: 0; batch classifier loss: 0.464550; batch adversarial loss: 0.605917\n",
      "epoch 15; iter: 200; batch classifier loss: 0.439958; batch adversarial loss: 0.573476\n",
      "epoch 16; iter: 0; batch classifier loss: 0.510620; batch adversarial loss: 0.641154\n",
      "epoch 16; iter: 200; batch classifier loss: 0.462011; batch adversarial loss: 0.624357\n",
      "epoch 17; iter: 0; batch classifier loss: 0.493129; batch adversarial loss: 0.560822\n",
      "epoch 17; iter: 200; batch classifier loss: 0.502133; batch adversarial loss: 0.629306\n",
      "epoch 18; iter: 0; batch classifier loss: 0.383448; batch adversarial loss: 0.648267\n",
      "epoch 18; iter: 200; batch classifier loss: 0.488405; batch adversarial loss: 0.580682\n",
      "epoch 19; iter: 0; batch classifier loss: 0.378838; batch adversarial loss: 0.593255\n",
      "epoch 19; iter: 200; batch classifier loss: 0.478819; batch adversarial loss: 0.578964\n",
      "epoch 20; iter: 0; batch classifier loss: 0.440038; batch adversarial loss: 0.599725\n",
      "epoch 20; iter: 200; batch classifier loss: 0.543063; batch adversarial loss: 0.621477\n",
      "epoch 21; iter: 0; batch classifier loss: 0.371448; batch adversarial loss: 0.558671\n",
      "epoch 21; iter: 200; batch classifier loss: 0.445343; batch adversarial loss: 0.610377\n",
      "epoch 22; iter: 0; batch classifier loss: 0.434380; batch adversarial loss: 0.598373\n",
      "epoch 22; iter: 200; batch classifier loss: 0.362297; batch adversarial loss: 0.659991\n",
      "epoch 23; iter: 0; batch classifier loss: 0.397963; batch adversarial loss: 0.586581\n",
      "epoch 23; iter: 200; batch classifier loss: 0.448995; batch adversarial loss: 0.576610\n",
      "epoch 24; iter: 0; batch classifier loss: 0.447692; batch adversarial loss: 0.578373\n",
      "epoch 24; iter: 200; batch classifier loss: 0.407920; batch adversarial loss: 0.592356\n",
      "epoch 25; iter: 0; batch classifier loss: 0.454302; batch adversarial loss: 0.598211\n",
      "epoch 25; iter: 200; batch classifier loss: 0.461321; batch adversarial loss: 0.605814\n",
      "epoch 26; iter: 0; batch classifier loss: 0.457989; batch adversarial loss: 0.615815\n",
      "epoch 26; iter: 200; batch classifier loss: 0.481574; batch adversarial loss: 0.608432\n",
      "epoch 27; iter: 0; batch classifier loss: 0.379744; batch adversarial loss: 0.593280\n",
      "epoch 27; iter: 200; batch classifier loss: 0.469927; batch adversarial loss: 0.655162\n",
      "epoch 28; iter: 0; batch classifier loss: 0.339607; batch adversarial loss: 0.556663\n",
      "epoch 28; iter: 200; batch classifier loss: 0.391201; batch adversarial loss: 0.656279\n",
      "epoch 29; iter: 0; batch classifier loss: 0.407124; batch adversarial loss: 0.576445\n",
      "epoch 29; iter: 200; batch classifier loss: 0.371261; batch adversarial loss: 0.583821\n",
      "epoch 30; iter: 0; batch classifier loss: 0.392815; batch adversarial loss: 0.587886\n",
      "epoch 30; iter: 200; batch classifier loss: 0.330804; batch adversarial loss: 0.591532\n",
      "epoch 31; iter: 0; batch classifier loss: 0.417632; batch adversarial loss: 0.633349\n",
      "epoch 31; iter: 200; batch classifier loss: 0.395008; batch adversarial loss: 0.583829\n",
      "epoch 32; iter: 0; batch classifier loss: 0.464306; batch adversarial loss: 0.633599\n",
      "epoch 32; iter: 200; batch classifier loss: 0.410680; batch adversarial loss: 0.624893\n",
      "epoch 33; iter: 0; batch classifier loss: 0.436253; batch adversarial loss: 0.634638\n",
      "epoch 33; iter: 200; batch classifier loss: 0.433610; batch adversarial loss: 0.545494\n",
      "epoch 34; iter: 0; batch classifier loss: 0.465634; batch adversarial loss: 0.541166\n",
      "epoch 34; iter: 200; batch classifier loss: 0.452528; batch adversarial loss: 0.525866\n",
      "epoch 35; iter: 0; batch classifier loss: 0.467947; batch adversarial loss: 0.626848\n",
      "epoch 35; iter: 200; batch classifier loss: 0.472747; batch adversarial loss: 0.609376\n",
      "epoch 36; iter: 0; batch classifier loss: 0.411472; batch adversarial loss: 0.587808\n",
      "epoch 36; iter: 200; batch classifier loss: 0.448827; batch adversarial loss: 0.579523\n",
      "epoch 37; iter: 0; batch classifier loss: 0.445256; batch adversarial loss: 0.652318\n",
      "epoch 37; iter: 200; batch classifier loss: 0.433323; batch adversarial loss: 0.595378\n",
      "epoch 38; iter: 0; batch classifier loss: 0.460859; batch adversarial loss: 0.561135\n",
      "epoch 38; iter: 200; batch classifier loss: 0.427011; batch adversarial loss: 0.590930\n",
      "epoch 39; iter: 0; batch classifier loss: 0.465886; batch adversarial loss: 0.596297\n",
      "epoch 39; iter: 200; batch classifier loss: 0.446854; batch adversarial loss: 0.572900\n",
      "epoch 40; iter: 0; batch classifier loss: 0.503895; batch adversarial loss: 0.600315\n",
      "epoch 40; iter: 200; batch classifier loss: 0.382876; batch adversarial loss: 0.597768\n",
      "epoch 41; iter: 0; batch classifier loss: 0.476227; batch adversarial loss: 0.595916\n",
      "epoch 41; iter: 200; batch classifier loss: 0.335235; batch adversarial loss: 0.667594\n",
      "epoch 42; iter: 0; batch classifier loss: 0.441358; batch adversarial loss: 0.569146\n",
      "epoch 42; iter: 200; batch classifier loss: 0.444636; batch adversarial loss: 0.602388\n",
      "epoch 43; iter: 0; batch classifier loss: 0.429892; batch adversarial loss: 0.617347\n",
      "epoch 43; iter: 200; batch classifier loss: 0.494456; batch adversarial loss: 0.582319\n",
      "epoch 44; iter: 0; batch classifier loss: 0.394850; batch adversarial loss: 0.633642\n",
      "epoch 44; iter: 200; batch classifier loss: 0.422038; batch adversarial loss: 0.604194\n",
      "epoch 45; iter: 0; batch classifier loss: 0.394931; batch adversarial loss: 0.652493\n",
      "epoch 45; iter: 200; batch classifier loss: 0.426966; batch adversarial loss: 0.632460\n",
      "epoch 46; iter: 0; batch classifier loss: 0.627063; batch adversarial loss: 0.550454\n",
      "epoch 46; iter: 200; batch classifier loss: 0.457183; batch adversarial loss: 0.580357\n",
      "epoch 47; iter: 0; batch classifier loss: 0.436961; batch adversarial loss: 0.584653\n",
      "epoch 47; iter: 200; batch classifier loss: 0.388905; batch adversarial loss: 0.626712\n",
      "epoch 48; iter: 0; batch classifier loss: 0.405610; batch adversarial loss: 0.590386\n",
      "epoch 48; iter: 200; batch classifier loss: 0.391778; batch adversarial loss: 0.606033\n",
      "epoch 49; iter: 0; batch classifier loss: 0.383817; batch adversarial loss: 0.564715\n",
      "epoch 49; iter: 200; batch classifier loss: 0.431373; batch adversarial loss: 0.628837\n"
     ]
    }
   ],
   "source": [
    "def get_plain_and_debaised_model():\n",
    "    ## trains on whole dataset/ no train-test split in this function\n",
    "    dataset_orig= load_preproc_data_adult()\n",
    "    dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "    privileged_groups = [{'sex': 1}]\n",
    "    unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "    sess_plain = tf.Session()\n",
    "    plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess_plain)\n",
    "\n",
    "    plain_model.fit(dataset_orig_train)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    sess_debiased = tf.Session()\n",
    "    debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess_debiased)\n",
    "\n",
    "    debiased_model.fit(dataset_orig_train)\n",
    "\n",
    "    return plain_model, debiased_model\n",
    "\n",
    "\n",
    "plain_model, debiased_model = get_plain_and_debaised_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_entry_adult(race, sex, age, education_years):\n",
    "    arr = np.array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "    arr[0][0] = race\n",
    "    arr[0][1] = sex\n",
    "    if age <= 19:\n",
    "        arr[0][2] = 1.\n",
    "    elif age <= 29:\n",
    "        arr[0][3] = 1.\n",
    "    elif age <= 39:\n",
    "        arr[0][4] = 1.\n",
    "    elif age <= 49:\n",
    "        arr[0][5] = 1.\n",
    "    elif age <= 59:\n",
    "        arr[0][6] = 1.\n",
    "    elif age <= 69:\n",
    "        arr[0][7] = 1.\n",
    "    else:\n",
    "        arr[0][8] = 1.\n",
    "\n",
    "    if education_years < 6:\n",
    "        arr[0][16] = 1.\n",
    "    elif education_years == 6:\n",
    "        arr[0][9] = 1.\n",
    "    elif education_years == 7:\n",
    "        arr[0][10] = 1.\n",
    "    elif education_years == 8:\n",
    "        arr[0][11] = 1.\n",
    "    elif education_years == 9:\n",
    "        arr[0][12] = 1.\n",
    "    elif education_years == 10:\n",
    "        arr[0][13] = 1.\n",
    "    elif education_years == 11:\n",
    "        arr[0][14] = 1.\n",
    "    elif education_years == 12:\n",
    "        arr[0][15] = 1.\n",
    "    else:\n",
    "        arr[0][17] = 1.\n",
    "    \n",
    "    dataset_replaced_data = load_preproc_data_adult()\n",
    "    dataset_replaced_data.features = arr\n",
    "    dataset_replaced_data.age = age\n",
    "    dataset_replaced_data.edu = education_years\n",
    "    return dataset_replaced_data\n",
    "\n",
    "# IDK what the race and sex is - that is, idk what sex = 1 means or race = 0\n",
    "# not sure where to find that\n",
    "example_input = create_single_entry_adult(1, 0, 65, 13)\n",
    "# we can make a script that prompts a user to input sex, race, age, and education and input to this^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that a 65 year old (race=1.0) (sex=0.0) with 13 years of education DOES NOT have an income greater than 50k.\n"
     ]
    }
   ],
   "source": [
    "def predict_single_income(model, user_input):\n",
    "    pred = model.predict(user_input).labels[0][0]\n",
    "    \n",
    "    race_print = f\"(race={user_input.features[0][0]})\" # idk which value is which race so this is the placeholder \n",
    "    sex_print = f\"(sex={user_input.features[0][1]})\" # idk which value is which sex so this is the placeholder \n",
    "\n",
    "    if pred == 1.0:\n",
    "        print(f\"The model predicts that a {user_input.age} year old {race_print} {sex_print} with {user_input.edu} years of education DOES have an income greater than 50k.\")\n",
    "    elif pred == 0.0:\n",
    "        print(f\"The model predicts that a {user_input.age} year old {race_print} {sex_print} with {user_input.edu} years of education DOES NOT have an income greater than 50k.\")\n",
    "\n",
    "predict_single_income(plain_model, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that a 65 year old (race=1.0) (sex=1.0) with 13 years of education DOES have an income greater than 50k.\n",
      "The model predicts that a 65 year old (race=1.0) (sex=0.0) with 13 years of education DOES NOT have an income greater than 50k.\n"
     ]
    }
   ],
   "source": [
    "input1 = create_single_entry_adult(1, 1, 65, 13)\n",
    "input2 = create_single_entry_adult(1, 0, 65, 13)\n",
    "\n",
    "predict_single_income(plain_model, input1)\n",
    "predict_single_income(plain_model, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that a 65 year old (race=1.0) (sex=1.0) with 11 years of education DOES NOT have an income greater than 50k.\n",
      "The model predicts that a 65 year old (race=1.0) (sex=0.0) with 11 years of education DOES NOT have an income greater than 50k.\n"
     ]
    }
   ],
   "source": [
    "input1 = create_single_entry_adult(1, 1, 65, 11)\n",
    "input2 = create_single_entry_adult(1, 0, 65, 11)\n",
    "\n",
    "predict_single_income(debiased_model, input1)\n",
    "predict_single_income(debiased_model, input2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('aif360')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19dfc7de0aa749caccab2babddb61847d88f8a91170dc0e706a22d32c3d8605a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
